import os
from pathlib import Path
import time
import numpy as np
from tqdm import tqdm

import torch
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
from sklearn.model_selection import StratifiedShuffleSplit
import torchaudio.transforms as T

# â”€â”€ 0. ê³µí†µ í´ë˜ìŠ¤ â†’ ë¼ë²¨ ë§¤í•‘ í•œ ë²ˆë§Œ ë§Œë“ ë‹¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from pathlib import Path
from collections import Counter

base_dir = Path("mel_specs aiu")
SUFFIXES = ["_a", "_i", "_u"]
N_MELS = 128

# â€˜normal_aâ€™ â†’ â€˜normalâ€™, â€˜polyp_aâ€™ â†’ â€˜polypâ€™ â€¦
class_names = {
    d.name[: -len(sfx)]
    for d in base_dir.iterdir() if d.is_dir()
    for sfx in SUFFIXES if d.name.endswith(sfx)
}
class_to_idx = {name: idx for idx, name in enumerate(sorted(class_names))}
print("í´ë˜ìŠ¤ â†’ ë¼ë²¨:", class_to_idx)   # ì˜ˆ: {'normal': 0, 'polyp': 1, ...}

def collect_paths_by_suffix(base: Path, suffix: str):
    paths, labels, ids = [], [], []                     # â˜… ID ëª©ë¡ë„ í•¨ê»˜
    for d in base.iterdir():
        if d.is_dir() and d.name.endswith(suffix):
            class_name = d.name[: -len(suffix)]
            lbl = class_to_idx[class_name]
            for f in d.rglob("*.npy"):
                paths.append(str(f))
                labels.append(lbl)
                ids.append(get_patient_id(f))           # â˜… ID ì¶”ì¶œ
    return paths, labels, ids

# 1. ëª¨ë¸ í´ë˜ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°
# (ì‚¬ì „ ì¤€ë¹„: 'CNN mel spect.py' -> 'model_cnn.py'ë¡œ íŒŒì¼ëª… ë³€ê²½ ê¶Œì¥)
try:
    from model_cnn import Mel2DCNN
except ImportError:
    print("[ì˜¤ë¥˜] 'model_cnn.py' íŒŒì¼ì—ì„œ 'Mel2DCNN' í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ëª¨ë¸ ì •ì˜ê°€ í¬í•¨ëœ íŒŒì¼ì˜ ì´ë¦„ì„ 'model_cnn.py'ë¡œ ë³€ê²½í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()

# ---------------------------------
# 2. PyTorch Dataset ì •ì˜
# ---------------------------------
class SoundDataset(Dataset):
    """
    Mel-spectrogram .npy íŒŒì¼ë“¤ì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ PyTorch Dataset ì…ë‹ˆë‹¤.
    """
    def __init__(self, file_paths, labels):
        self.file_paths = file_paths
        self.labels = labels

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        path = self.file_paths[idx]
        label = self.labels[idx]
        
        # .npy íŒŒì¼ì—ì„œ Mel-spectrogram ë¶ˆëŸ¬ì˜¤ê¸°
        # shape: (n_mels, n_frames)
        mel_spec = np.load(path)
        
        # PyTorch í…ì„œë¡œ ë³€í™˜
        mel_spec_tensor = torch.from_numpy(mel_spec).float()
        
        return mel_spec_tensor, label

def save_augmented_minority_pairwise(file_paths, labels, ids, suffixes,
                                     train_pids, minor_pairs, pair_cnt, max_cnt,
                                     n_aug_per_file=2,
                                     freq_param=13, time_param=15):
    """
    (label, suffix) ë‹¨ìœ„ë¡œ max_cntê¹Œì§€ ì¦ê°•.
    pair_cnt: {(lbl, suf): ì›ë³¸ ê°œìˆ˜}
    minor_pairs: ë¶€ì¡±í•œ pair ì§‘í•©
    """
    fmask = T.FrequencyMasking(freq_param)
    tmask = T.TimeMasking(time_param)

    per_pair_generated = Counter()
    aug_cnt = 0

    for path, lbl, pid, suf in zip(file_paths, labels, ids, suffixes):
        pair = (lbl, suf)

        # 1) Train í™˜ì/ì†Œìˆ˜ pairë§Œ ëŒ€ìƒ
        if pid not in train_pids or pair not in minor_pairs:
            continue

        # 2) ëª©í‘œ ìˆ˜ëŸ‰ ë„ë‹¬ ì‹œ skip
        if pair_cnt[pair] + per_pair_generated[pair] >= max_cnt:
            continue

        mel = np.load(path).astype(np.float32)
        stem, ext = os.path.splitext(path)

        for _ in range(n_aug_per_file):
            if pair_cnt[pair] + per_pair_generated[pair] >= max_cnt:
                break

            aug = tmask(fmask(torch.from_numpy(mel))).numpy()
            new_path = f"{stem}_aug{per_pair_generated[pair]}{ext}"

            if os.path.exists(new_path):
                per_pair_generated[pair] += 1
                continue

            np.save(new_path, aug)
            per_pair_generated[pair] += 1
            aug_cnt += 1

    print("ìŒë³„ ì¶”ê°€ ìˆ˜:", per_pair_generated)
    return aug_cnt


def clean_aug_files(root="mel_specs aiu", postfix="_aug"):
    removed = 0
    for f in Path(root).rglob(f"*{postfix}*.npy"):
        f.unlink()
        removed += 1
    print(f"ğŸ—‘  ê¸°ì¡´ ì¦ê°• íŒŒì¼ {removed}ê°œ ì‚­ì œ ì™„ë£Œ")
# ---------------------------------
# 3. íŒ¨ë”©ì„ ìœ„í•œ Custom Collate í•¨ìˆ˜
# ---------------------------------
def pad_collate_fn(batch):
    """
    ê¸¸ì´ê°€ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ë“¤ì„ íŒ¨ë”©í•˜ì—¬ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ë§Œë“­ë‹ˆë‹¤.
    ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë§ì¶° ë‚˜ë¨¸ì§€ ì‹œí€€ìŠ¤ë“¤ì˜ ë’·ë¶€ë¶„ì„ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
    
    Args:
        batch: (spectrogram_tensor, label) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        (padded_spectrograms_tensor, labels_tensor) íŠœí”Œ
    """
    # ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    batch.sort(key=lambda x: x[0].shape[1], reverse=True)
    
    spectrograms, labels = zip(*batch)
    
    # ë°°ì¹˜ ë‚´ ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´
    max_len = spectrograms[0].shape[1]
    
    # ëª¨ë“  ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ max_lenì— ë§ì¶° íŒ¨ë”©
    padded_spectrograms = []
    for spec in spectrograms:
        # specì˜ shape: (n_mels, n_frames)
        # ë‘ ë²ˆì§¸ ì°¨ì›(n_frames)ì„ íŒ¨ë”©í•´ì•¼ í•¨
        pad_len = max_len - spec.shape[1]
        # F.padëŠ” ë§ˆì§€ë§‰ ì°¨ì›ë¶€í„° íŒ¨ë”©ì„ ì ìš©. (ì™¼ìª½ íŒ¨ë”©, ì˜¤ë¥¸ìª½ íŒ¨ë”©)
        padded_spec = F.pad(spec, (0, pad_len), 'constant', 0)
        padded_spectrograms.append(padded_spec)
        
    # íŒ¨ë”©ëœ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ë“¤ì„ í•˜ë‚˜ì˜ í…ì„œë¡œ í•©ì¹¨
    spectrograms_tensor = torch.stack(padded_spectrograms)
    labels_tensor = torch.tensor(labels, dtype=torch.long)
    
    return spectrograms_tensor, labels_tensor

# ---------------------------------
# 5. í›ˆë ¨, í‰ê°€, í”¼ì²˜ ì¶”ì¶œ í•¨ìˆ˜
# ---------------------------------
def train_one_epoch(model, data_loader, criterion, optimizer, device):
    """í•œ ì—í­(epoch) ë™ì•ˆ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤."""
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_samples = 0

    for inputs, labels in tqdm(data_loader, desc="Training"):
        inputs, labels = inputs.to(device), labels.to(device)

        # ìˆœì „íŒŒ
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # ì—­ì „íŒŒ ë° ìµœì í™”
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # í†µê³„ ê¸°ë¡
        total_loss += loss.item() * inputs.size(0)
        _, predicted = torch.max(outputs.data, 1)
        total_samples += labels.size(0)
        correct_predictions += (predicted == labels).sum().item()

    epoch_loss = total_loss / total_samples
    epoch_acc = correct_predictions / total_samples
    return epoch_loss, epoch_acc

from sklearn.metrics import classification_report # íŒŒì¼ ìƒë‹¨ì— ì´ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”.

def evaluate(model, data_loader, criterion, device):
    """ëª¨ë¸ì„ í‰ê°€í•˜ê³ , ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.""" # (ì„¤ëª… ë³€ê²½)
    model.eval()
    total_loss = 0
    correct_predictions = 0
    total_samples = 0
    
    all_labels = [] # ì‹¤ì œ ë¼ë²¨ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ (ì¶”ê°€)
    all_preds = []  # ì˜ˆì¸¡ ë¼ë²¨ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ (ì¶”ê°€)

    with torch.no_grad():
        for inputs, labels in tqdm(data_loader, desc="Evaluating"):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            total_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            total_samples += labels.size(0)
            correct_predictions += (predicted == labels).sum().item()
            
            # --- â–¼â–¼â–¼ ì¶”ê°€ëœ ë¶€ë¶„ â–¼â–¼â–¼ ---
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())
            # --- â–²â–²â–² ì¶”ê°€ëœ ë¶€ë¶„ â–²â–²â–² ---

    epoch_loss = total_loss / total_samples
    epoch_acc = correct_predictions / total_samples
    
    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜ (ë³€ê²½)
    return epoch_loss, epoch_acc, np.array(all_labels), np.array(all_preds)

def get_feature_extractor(model_path, num_classes, n_mels, device):
    """
    í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ë¥¼ ì œê±°í•œ í”¼ì²˜ ì¶”ì¶œê¸° ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ë¨¼ì € ì „ì²´ ëª¨ë¸ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    model = Mel2DCNN(num_classes=num_classes, n_mels=n_mels)
    
    # í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    model.load_state_dict(torch.load(model_path, map_location=device))
    
    # ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´(fc2)ë¥¼ Identity ë ˆì´ì–´ë¡œ êµì²´í•©ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•˜ë©´ fc1ì˜ ì¶œë ¥ì´ ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ì´ ë©ë‹ˆë‹¤.
    model.fc2 = torch.nn.Identity()
    
    model.to(device)
    model.eval() # í”¼ì²˜ ì¶”ì¶œì€ í•­ìƒ í‰ê°€ ëª¨ë“œì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    return model

def extract_features(feature_extractor, data_loader, device):
    """
    í”¼ì²˜ ì¶”ì¶œê¸° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ì „ì²´ì˜ í”¼ì²˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    all_features = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in tqdm(data_loader, desc="Extracting Features"):
            inputs = inputs.to(device)
            
            # ëª¨ë¸ì„ í†µê³¼ì‹œì¼œ í”¼ì²˜ë¥¼ ì–»ìŠµë‹ˆë‹¤.
            features = feature_extractor(inputs)
            
            all_features.append(features.cpu().numpy())
            all_labels.append(labels.numpy())
            
    # ë¦¬ìŠ¤íŠ¸ë“¤ì„ í•˜ë‚˜ì˜ numpy ë°°ì—´ë¡œ í•©ì¹©ë‹ˆë‹¤.
    all_features = np.concatenate(all_features, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)
    
    return all_features, all_labels

import re
from pathlib import Path

ID_RE = re.compile(r"^(\d+)")     # íŒŒì¼ëª… ì²« ì—°ì† ìˆ«ì

def get_patient_id(file_path: str | Path) -> str:
    """Path â†’ Patient ID (str). ì˜ˆ: '1251372a-...' â†’ '1251372'"""
    stem = Path(file_path).stem       # í™•ì¥ì ì œê±°
    m = ID_RE.match(stem)
    if not m:
        raise ValueError(f"ID ì¶”ì¶œ ì‹¤íŒ¨: {file_path}")
    return m.group(1)




# ---------------------------------
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# ---------------------------------
if __name__ == '__main__':
    clean_aug_files()

    BASE_DATA_DIR = "mel_specs aiu"
    SUFFIXES      = ["_a", "_i", "_u"]
    MODEL_TMPL    = "best_cnn_model{}.pth"
    BATCH_SIZE = 16; NUM_EPOCHS = 20; LR = 1e-3; N_MELS = 128
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ---------- 1. í™˜ì ë¶„í•  ----------
    all_paths_by_suf, all_labels_by_suf, all_ids_by_suf = {}, {}, {}
    all_paths, all_labels, all_ids, all_sufs = [], [], [], []

    for suf in SUFFIXES:
        p, l, i = collect_paths_by_suffix(Path(BASE_DATA_DIR), suf)
        all_paths_by_suf[suf], all_labels_by_suf[suf], all_ids_by_suf[suf] = p, l, i

        all_paths  += p
        all_labels += l
        all_ids    += i
        all_sufs   += [suf] * len(p)

    # í™˜ì ëŒ€í‘œ ë¼ë²¨ ì‚°ì¶œ
    df = pd.DataFrame({"pid": all_ids, "label": all_labels})
    patient_major = df.groupby("pid")["label"].agg(lambda x: x.mode()[0])
    pids = patient_major.index.to_numpy()
    pid_labels = patient_major.to_numpy()

    # Stratified patient split (60/20/20)
    sss1 = StratifiedShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
    trv_idx, te_idx = next(sss1.split(pids, pid_labels))
    pids_trv, pids_te = pids[trv_idx], pids[te_idx]

    labels_trv = patient_major.loc[pids_trv].to_numpy()
    sss2 = StratifiedShuffleSplit(test_size=0.25, n_splits=1, random_state=42)
    tr_idx, va_idx = next(sss2.split(pids_trv, labels_trv))
    pids_tr, pids_va = pids_trv[tr_idx], pids_trv[va_idx]

    train_pids, val_pids, test_pids = set(pids_tr), set(pids_va), set(pids_te)
    assert train_pids.isdisjoint(val_pids) and train_pids.isdisjoint(test_pids) and val_pids.isdisjoint(test_pids)
    print(f"âœ… í™˜ì ë¶„í•  ì™„ë£Œ: Train {len(train_pids)}, Val {len(val_pids)}, Test {len(test_pids)} ëª…")

    # ---------- 2. (label, suffix) ë‹¨ìœ„ë¡œ ì¦ê°• ê¸°ì¤€ ê³„ì‚° ----------
    train_mask     = [get_patient_id(p) in train_pids for p in all_paths]
    train_labels   = [l for l, m in zip(all_labels, train_mask) if m]
    train_suffixes = [s for s, m in zip(all_sufs,  train_mask) if m]

    pair_cnt   = Counter(zip(train_labels, train_suffixes))
    max_cnt    = max(pair_cnt.values())
    minor_pairs = {k for k, v in pair_cnt.items() if v < max_cnt}

    print("pair_cnt:", pair_cnt)
    print("max_cnt :", max_cnt)
    print("minor_pairs:", minor_pairs)

    # ---------- 3. ì¦ê°• ì‹¤í–‰ ----------
    aug_num = save_augmented_minority_pairwise(
        all_paths, all_labels, all_ids, all_sufs,
        train_pids, minor_pairs, pair_cnt, max_cnt,
        n_aug_per_file=2, freq_param=13, time_param=15
    )
    print(f"ğŸ†• ì†Œìˆ˜ (label,suffix) ì¦ê°• íŒŒì¼ {aug_num}ê°œ ì €ì¥ ì™„ë£Œ")

    # ì¦ê°• í›„ ê²½ë¡œ/ë¼ë²¨/ID ì¬ìˆ˜ì§‘
    for suf in SUFFIXES:
        p, l, i = collect_paths_by_suffix(Path(BASE_DATA_DIR), suf)
        all_paths_by_suf[suf]  = p
        all_labels_by_suf[suf] = l
        all_ids_by_suf[suf]    = i

    # ---------- 4. ëª¨ìŒë³„ í•™ìŠµ ë£¨í”„ ----------
    for suf in SUFFIXES:
        print(f"\n================  [{suf}] ë°ì´í„° ì¤€ë¹„  ================")
        file_paths, labels, ids = all_paths_by_suf[suf], all_labels_by_suf[suf], all_ids_by_suf[suf]
        if not file_paths:
            print(f"[ê²½ê³ ] '{suf}' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); continue

        tr_p, tr_l = [p for i, p in enumerate(file_paths) if ids[i] in train_pids], [l for i, l in enumerate(labels) if ids[i] in train_pids]
        va_p, va_l = [p for i, p in enumerate(file_paths) if ids[i] in val_pids],   [l for i, l in enumerate(labels) if ids[i] in val_pids]
        te_p, te_l = [p for i, p in enumerate(file_paths) if ids[i] in test_pids],  [l for i, l in enumerate(labels) if ids[i] in test_pids]
        print(f"ìƒ˜í”Œ ë¶„í• : Train {len(tr_p)}, Val {len(va_p)}, Test {len(te_p)} ê°œ")

        tr_ds = SoundDataset(tr_p, tr_l)
        va_ds = SoundDataset(va_p, va_l)
        te_ds = SoundDataset(te_p, te_l)

        # (ì„ íƒ) ì™„ì „ ì¬í˜„ì„±: generator ì§€ì •
        g = torch.Generator().manual_seed(42)
        tr_loader = DataLoader(tr_ds, BATCH_SIZE, True,  collate_fn=pad_collate_fn, generator=g)
        va_loader = DataLoader(va_ds, BATCH_SIZE, False, collate_fn=pad_collate_fn)
        te_loader = DataLoader(te_ds, BATCH_SIZE, False, collate_fn=pad_collate_fn)

        model = Mel2DCNN(num_classes=len(class_to_idx), n_mels=N_MELS).to(DEVICE)
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=LR)

        best_acc = 0.0
        for epoch in range(NUM_EPOCHS):
            tr_loss, tr_acc = train_one_epoch(model, tr_loader, criterion, optimizer, DEVICE)
            va_loss, va_acc, _, _ = evaluate(model, va_loader, criterion, DEVICE)
            if va_acc > best_acc:
                best_acc = va_acc
                torch.save(model.state_dict(), MODEL_TMPL.format(suf))
            print(f"[{suf}][E{epoch+1}] train {tr_acc:.3f} | val {va_acc:.3f}")

        # --- í…ŒìŠ¤íŠ¸ ---
        print(f"\n--- [{suf}] ìµœì¢… ëª¨ë¸ ìƒì„¸ í‰ê°€ ---")
        model.load_state_dict(torch.load(MODEL_TMPL.format(suf), map_location=DEVICE))
        te_loss, te_acc, y_true, y_pred = evaluate(model, te_loader, criterion, DEVICE)
        print(f"âœ… [{suf}] ìµœì¢… test acc = {te_acc:.4f}")

        target_names = sorted(class_to_idx, key=class_to_idx.get)
        report = classification_report(y_true, y_pred, target_names=target_names, zero_division=0)
        print("\n[Classification Report]")
        print(report)

        # --- í”¼ì²˜ ì¶”ì¶œ ---
        feature_extractor = get_feature_extractor(MODEL_TMPL.format(suf), len(class_to_idx), N_MELS, DEVICE)
        full_ds = SoundDataset(file_paths, labels)
        full_ld = DataLoader(full_ds, BATCH_SIZE, False, collate_fn=pad_collate_fn)
        feats, labs = extract_features(feature_extractor, full_ld, DEVICE)
        ids_all = [get_patient_id(p) for p in file_paths]

        np.save(f"cnn_features{suf}.npy", feats)
        np.save(f"cnn_labels{suf}.npy",   labs)
        np.save(f"cnn_ids{suf}.npy",      np.array(ids_all))
        print(f"[{suf}] í”¼ì²˜ shape {feats.shape} ì €ì¥ ì™„ë£Œ")

    np.savez(
        "patient_split_seed(agu).npz",
        train=list(train_pids),
        val=list(val_pids),
        test=list(test_pids)
    )
    print("âœ… í™˜ì ë¶„í•  ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ patient_split_seed(agu).npz")
